{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "#状態を表すクラス\n",
    "class State():\n",
    "    def __init__(self, row=-1, column=-1):\n",
    "        self.column = column\n",
    "        self.row = row\n",
    "\n",
    "    #状態の表現\n",
    "    def __repr__(self):\n",
    "        return \"<State:[{}, {}]>\".format(self.row, self.column)\n",
    "    #クローン生成\n",
    "    def clone(self):\n",
    "        return State(self.row, self.column)\n",
    "    #ハッシュ型のクローン?\n",
    "    def __hash__(self):\n",
    "        return hash((self.row, self.column))\n",
    "\n",
    "    #同値判定\n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.column == other.column\n",
    "\n",
    "#行動の定義\n",
    "class Action(Enum):\n",
    "    UP = 1\n",
    "    DOWN = -1\n",
    "    LEFT = 2\n",
    "    RIGHT = -2\n",
    "    STAY = 0\n",
    "\n",
    "#Agent_Base\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.observer = Observer(env)\n",
    "        #self.planner = planner\n",
    "        self.actions = env.actions()\n",
    "\n",
    "    def act(self, state): #TODO 行動確率を返したい．行動を返す関数 & Plannerに行動選択も委託しよう\n",
    "        a = random.choice(self.actions)\n",
    "        return a\n",
    "\n",
    "    def learn(self): #TODO 学習方法について(引数には必要な要素) & Plannerに役割を分散できるか．\n",
    "        pass\n",
    "        \n",
    "#環境の情報取得のためのクラス\n",
    "class Observer():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def get_state(self): #状態観測\n",
    "        return self.env.agent_state\n",
    "\n",
    "    def reset(self): \n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "    def transform(self, state): #観測情報扱いやすい形に変換する\n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "class Planner():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def learn(self, state, action, reward): #学習方法\n",
    "        pass\n",
    "\n",
    "    def policy(self): #行動選択\n",
    "        pass\n",
    "\n",
    "class Logger(): #記録用クラス\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def log_func(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment関連のクラス\n",
    "\n",
    "#ライブラリimport\n",
    "import numpy as np\n",
    "\n",
    "class Maze():\n",
    "\n",
    "    def __init__(self, grid):\n",
    "\n",
    "        self.grid = grid\n",
    "        self.agent_state = State()\n",
    "\n",
    "        self.default_reward = -0.04\n",
    "        self.collision_reward = -10 #マルチエージェントの時に使用\n",
    "    \n",
    "    @property\n",
    "    def row_length(self):\n",
    "        return len(self.grid)\n",
    "    @property\n",
    "    def column_length(self):\n",
    "        return len(self.grid[0])\n",
    "\n",
    "    def actions(self):\n",
    "        return [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT, Action.STAY]\n",
    "\n",
    "    #環境の初期化を行う\n",
    "    def reset(self):\n",
    "        row_len = self.row_length\n",
    "        self.agent_state = State(row_len - 1, 0) #エージェントの位置を初期化 *とりまこれだけ\n",
    "        return self.agent_state\n",
    "        \n",
    "    #遷移のための関数    return 遷移確率\n",
    "    def transit_func(self, state, action):\n",
    "        transition_probs = {}\n",
    "        #動けないときは空の辞書\n",
    "        if not self.can_action_at(state):\n",
    "            return transition_probs\n",
    "\n",
    "        for a in self.actions:\n",
    "            prob = 0\n",
    "            #選んだ行動を確実にとる\n",
    "            if a == action:\n",
    "                prob = 1\n",
    "            \n",
    "            next_state = self._move(state, a)\n",
    "            if next_state not in transition_probs:\n",
    "                transition_probs[next_state] = prob\n",
    "            else:\n",
    "                transition_probs[next_state] += prob\n",
    "        return transition_probs\n",
    "    \n",
    "    #遷移を行う\n",
    "    def transit(self, state, action): #遷移確率をエージェントから獲得する．\n",
    "        transition_probs = self.transit_func(state, action)\n",
    "        if len(transition_probs) == 0:\n",
    "            return None, None, True\n",
    "\n",
    "        next_states = []\n",
    "        probs = []\n",
    "\n",
    "        for s in transition_probs:\n",
    "            next_states.append(s)\n",
    "            probs.append(transition_probs[s])\n",
    "\n",
    "        #おそらく選択行動がそのまま反映されるはず\n",
    "        next_state = np.random.choice(next_states, p=probs)\n",
    "\n",
    "        #報酬獲得と終了判定\n",
    "        reward, done = self.reward_func(next_state)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    #1 step turn\n",
    "    def step(self, action):\n",
    "        #TODO\n",
    "\n",
    "    #行動可能かの判定\n",
    "    def can_action_at(self, state):\n",
    "        #現在空マスにいるならTrue\n",
    "        if self.grid[state.row][state.column] == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _move(self, state, action):\n",
    "\n",
    "        if not self.can_action_at(state):\n",
    "            raise Exception(\"Can't move from here\")\n",
    "\n",
    "        next_state = state.clone()\n",
    "\n",
    "        #移動\n",
    "        if action == Action.UP:\n",
    "            next_state.row -= 1\n",
    "        if action == Action.DOWN:\n",
    "            next_state.row += 1\n",
    "        if action == Action.LEFT:\n",
    "            next_state.column -= 1\n",
    "        if action == Action.RIGHT:\n",
    "            next_state.column += 1\n",
    "        \n",
    "        #移動可能かのチェック 無理なら元に戻す\n",
    "        #迷路外に出たか\n",
    "        if not (0 <= next_state.row < self.row_length):\n",
    "            next_state = state\n",
    "        if not (0 <= next_state.column < self.column_length):\n",
    "            next_state = state\n",
    "\n",
    "        #壁にいるか\n",
    "        if self.grid[next_state.row][next_state.column] == 9:\n",
    "            next_state = state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    #報酬を与える関数\n",
    "    def reward_func(self, state):\n",
    "        reward = self.default_reward\n",
    "        done = False\n",
    "\n",
    "        attribute = self.grid[state.row][state.column]\n",
    "\n",
    "        if attribute == 1:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif attribute == -1:\n",
    "            reward = -10\n",
    "            done = True\n",
    "\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Maze' object has no attribute 'step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6b057e4ac215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-6b057e4ac215>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m      \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m      \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-6b057e4ac215>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Episode {}: Agent gets {} reward.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mone_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-6b057e4ac215>\u001b[0m in \u001b[0;36mone_episode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Maze' object has no attribute 'step'"
     ]
    }
   ],
   "source": [
    "#Training全体を受け持つクラス\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, agent, env, episode=1):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "\n",
    "        self.episode = episode\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.episode):\n",
    "            print(\"Episode {}: Agent gets {} reward.\".format(i, self.one_episode()))\n",
    "\n",
    "    def one_episode(self):\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = self.agent.act(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward \n",
    "\n",
    "    def learn(self):\n",
    "        pass\n",
    "\n",
    "    def log(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    #環境データ\n",
    "     grid = [\n",
    "         [0,0,0,1],\n",
    "         [0,9,0,-1],\n",
    "         [0,0,0,0]\n",
    "     ]\n",
    "\n",
    "     env = Maze(grid)\n",
    "     agent = Agent(env)\n",
    "\n",
    "     trainer = Trainer(agent, env)\n",
    "\n",
    "     trainer.train()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
