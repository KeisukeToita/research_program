{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# 状態を表すクラス\n",
    "class State():\n",
    "    def __init__(self, row=-1, column=-1):\n",
    "        self.column = column\n",
    "        self.row = row\n",
    "\n",
    "    # 状態の表現\n",
    "    def repr(self):\n",
    "        return \"<State:[{}, {}]>\".format(self.row, self.column)\n",
    "    # クローン生成\n",
    "\n",
    "    def clone(self):\n",
    "        return State(self.row, self.column)\n",
    "    # ハッシュ型のクローン?\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.row, self.column))\n",
    "\n",
    "    # 同値判定\n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.column == other.column\n",
    "\n",
    "# 行動の定義\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    STAY = 4\n",
    "\n",
    "# Agent_Base\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.observer = Observer(env)\n",
    "        self.actions = env.actions\n",
    "\n",
    "    def act(self, state):  # TODO 行動確率を返したい．行動を返す関数 & Plannerに行動選択も委託しよう\n",
    "        a = random.choice(self.actions)\n",
    "        return a\n",
    "\n",
    "    def learn(self):  # TODO 学習方法について(引数には必要な要素) & Plannerに役割を分散できるか．\n",
    "        pass\n",
    "\n",
    "\n",
    "class MonteCarloAgent(Agent):\n",
    "\n",
    "    def __init__(self, env, epsilon=0.1, gamma=0.9):\n",
    "        super().__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_log = []\n",
    "        self.experience_log = []\n",
    "        self.Q = defaultdict(lambda: [0] * len(self.actions))\n",
    "        self.N = defaultdict(lambda: [0] * len(self.actions))\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def act(self, now_state):  # epsilon-greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # ランダム行動選択\n",
    "            return trans_a(np.random.randint(len(self.actions)))\n",
    "        else:\n",
    "            # 現時点の良い行動を選択\n",
    "            if now_state in self.Q and sum(self.Q[now_state]) != 0:\n",
    "                return trans_a(np.argmax(self.Q[now_state]))\n",
    "            else:\n",
    "                return trans_a(np.random.randint(len(self.actions)))\n",
    "        \n",
    "\n",
    "    def init_log(self):  # experienceの初期化\n",
    "        self.experience_log = []\n",
    "\n",
    "    def experience_add(self, now_state, action, reward):  # listにどんどんappendしていく\n",
    "        #現在の状態からある行動によってどれだけの行動を得られたか\n",
    "        self.experience_log.append({\"state\": now_state, \"action\": trans_aton(action), \"reward\": reward})\n",
    "    \n",
    "    def reward_add(self, reward):\n",
    "        #reward_logに1エピソード分の報酬を追加\n",
    "        self.reward_log.append(reward)\n",
    "\n",
    "    def learn(self): #1エピソード終了後に発動\n",
    "        for i, x in enumerate(self.experience_log):\n",
    "            s, a = x[\"state\"], x[\"action\"]\n",
    "\n",
    "            # Calculate discounted future reward of s.\n",
    "            G, t = 0, 0\n",
    "            for j in range(i, len(self.experience_log)):\n",
    "                G += math.pow(self.gamma, t) * self.experience_log[j][\"reward\"]\n",
    "                t += 1\n",
    "\n",
    "            self.N[s][a] += 1  # count of s, a pair\n",
    "            alpha = 1 / self.N[s][a]\n",
    "            self.Q[s][a] += alpha * (G - self.Q[s][a])\n",
    "            \n",
    "\n",
    "def trans_ntoa(action_n):\n",
    "    if action_n == 0:\n",
    "        return Action.UP\n",
    "    if action_n == 1:\n",
    "        return Action.DOWN\n",
    "    if action_n == 2:\n",
    "        return Action.LEFT\n",
    "    if action_n == 3:\n",
    "        return Action.RIGHT\n",
    "    if action_n == 4:\n",
    "        return Action.STAY\n",
    "\n",
    "def trans_aton(action):\n",
    "    if action == Action.UP:\n",
    "        return 0\n",
    "    if action == Action.DOWN:\n",
    "        return 1\n",
    "    if action == Action.LEFT:\n",
    "        return 2\n",
    "    if action == Action.RIGHT:\n",
    "        return 3\n",
    "    if action == Action.STAY:\n",
    "        return 4\n",
    "\n",
    "# 環境の情報取得のためのクラス\n",
    "\n",
    "\n",
    "class Observer():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def get_state(self):  # 状態観測\n",
    "        return self.env.agent_state\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def transform(self, state):  # 観測情報扱いやすい形に変換する\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class Planner():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def learn(self, state, action, reward):  # 学習方法\n",
    "        pass\n",
    "\n",
    "    def policy(self):  # 行動選択\n",
    "        pass\n",
    "\n",
    "\n",
    "class LogShow():  # 記録用クラス\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        pass\n",
    "\n",
    "    def log_func(self):\n",
    "        pass\n",
    "\n",
    "    def show_q_value(self, Q):\n",
    "        nrow = self.env.row_length\n",
    "        ncol = self.env.column_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment関連のクラス\n",
    "\n",
    "#ライブラリimport\n",
    "import numpy as np\n",
    "\n",
    "class Maze():\n",
    "\n",
    "    def __init__(self, grid):\n",
    "\n",
    "        self.grid = grid\n",
    "        \n",
    "        self.agent_state = State()\n",
    "        self.init_agent_state = State(0,0)\n",
    "\n",
    "        self.default_reward = -0.04\n",
    "        self.collision_reward = -10 #マルチエージェントの時に使用\n",
    "    \n",
    "    @property\n",
    "    def row_length(self):\n",
    "        return len(self.grid)\n",
    "    @property\n",
    "    def column_length(self):\n",
    "        return len(self.grid[0])\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT, Action.STAY]\n",
    "\n",
    "    #環境の初期化を行う\n",
    "    def reset(self):\n",
    "        self.agent_state = self.init_agent_state #エージェントの位置を初期化 *とりまこれだけ\n",
    "        return self.agent_state\n",
    "        \n",
    "    #遷移のための関数    return 遷移確率\n",
    "    def transit_func(self, state, action):\n",
    "        transition_probs = {}\n",
    "        #動けないときは空の辞書\n",
    "        if not self.can_action_at(state):\n",
    "            return transition_probs\n",
    "\n",
    "        for a in self.actions:\n",
    "            prob = 0\n",
    "            #選んだ行動を確実にとる\n",
    "            if a == action:\n",
    "                prob = 1\n",
    "            \n",
    "            next_state = self._move(state, a)\n",
    "            if next_state not in transition_probs:\n",
    "                transition_probs[next_state] = prob\n",
    "            else:\n",
    "                transition_probs[next_state] += prob\n",
    "        return transition_probs\n",
    "    \n",
    "    #遷移を行う\n",
    "    def transit(self, state, action): #遷移確率をエージェントから獲得する．\n",
    "        transition_probs = self.transit_func(state, action)\n",
    "        if len(transition_probs) == 0:\n",
    "            return None, None, True\n",
    "\n",
    "        next_states = []\n",
    "        probs = []\n",
    "\n",
    "        for s in transition_probs:\n",
    "            next_states.append(s)\n",
    "            probs.append(transition_probs[s])\n",
    "\n",
    "        #おそらく選択行動がそのまま反映されるはず\n",
    "        next_state = np.random.choice(next_states, p=probs)\n",
    "\n",
    "        #報酬獲得と終了判定\n",
    "        reward, done = self.reward_func(next_state)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    #1 step turn\n",
    "    def step(self, action):\n",
    "        #TODO\n",
    "        next_state, reward, done = self.transit(self.agent_state, action)\n",
    "        \n",
    "        if next_state is not None:\n",
    "            self.agent_state = next_state\n",
    "            \n",
    "        return next_state, reward, done\n",
    "\n",
    "    #行動可能かの判定\n",
    "    def can_action_at(self, state):\n",
    "        #現在空マスにいるならTrue\n",
    "        if self.grid[state.row][state.column] == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _move(self, state, action):\n",
    "\n",
    "        if not self.can_action_at(state):\n",
    "            raise Exception(\"Can't move from here\")\n",
    "\n",
    "        next_state = state.clone()\n",
    "\n",
    "        #移動\n",
    "        if action == Action.UP:\n",
    "            next_state.row -= 1\n",
    "        if action == Action.DOWN:\n",
    "            next_state.row += 1\n",
    "        if action == Action.LEFT:\n",
    "            next_state.column -= 1\n",
    "        if action == Action.RIGHT:\n",
    "            next_state.column += 1\n",
    "        \n",
    "        #移動可能かのチェック 無理なら元に戻す\n",
    "        #迷路外に出たか\n",
    "        if not (0 <= next_state.row < self.row_length):\n",
    "            next_state = state\n",
    "        if not (0 <= next_state.column < self.column_length):\n",
    "            next_state = state\n",
    "\n",
    "        #壁にいるか\n",
    "        if self.grid[next_state.row][next_state.column] == 9:\n",
    "            next_state = state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    #報酬を与える関数\n",
    "    def reward_func(self, state):\n",
    "        reward = self.default_reward\n",
    "        done = False\n",
    "\n",
    "        attribute = self.grid[state.row][state.column]\n",
    "\n",
    "        if attribute == 1:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif attribute == -1:\n",
    "            reward = -10\n",
    "            done = True\n",
    "\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Agent gets -10.44 reward.\n",
      "Episode 51: Agent gets -3.6799999999998843 reward.\n",
      "Episode 101: Agent gets -10.96 reward.\n",
      "Episode 151: Agent gets -18.240000000000002 reward.\n",
      "Episode 201: Agent gets 9.48 reward.\n",
      "Episode 251: Agent gets 9.52 reward.\n",
      "Episode 301: Agent gets 9.84 reward.\n",
      "Episode 351: Agent gets 9.92 reward.\n",
      "Episode 401: Agent gets 9.92 reward.\n",
      "Episode 451: Agent gets 9.92 reward.\n",
      "Episode 501: Agent gets 9.88 reward.\n",
      "Episode 551: Agent gets 9.92 reward.\n",
      "Episode 601: Agent gets -10.28 reward.\n",
      "Episode 651: Agent gets 9.92 reward.\n",
      "Episode 701: Agent gets 9.84 reward.\n",
      "Episode 751: Agent gets 9.6 reward.\n",
      "Episode 801: Agent gets 9.88 reward.\n",
      "Episode 851: Agent gets 9.68 reward.\n",
      "Episode 901: Agent gets 9.84 reward.\n",
      "Episode 951: Agent gets 9.68 reward.\n",
      "Episode 1001: Agent gets 9.76 reward.\n",
      "Episode 1051: Agent gets 9.84 reward.\n",
      "Episode 1101: Agent gets 9.92 reward.\n",
      "Episode 1151: Agent gets 9.8 reward.\n",
      "Episode 1201: Agent gets 9.92 reward.\n",
      "Episode 1251: Agent gets 9.92 reward.\n",
      "Episode 1301: Agent gets 9.88 reward.\n",
      "Episode 1351: Agent gets 9.72 reward.\n",
      "Episode 1401: Agent gets 9.92 reward.\n",
      "Episode 1451: Agent gets 9.92 reward.\n",
      "Episode 1501: Agent gets 9.84 reward.\n",
      "Episode 1551: Agent gets 9.92 reward.\n",
      "Episode 1601: Agent gets 9.8 reward.\n",
      "Episode 1651: Agent gets 9.92 reward.\n",
      "Episode 1701: Agent gets 9.92 reward.\n",
      "Episode 1751: Agent gets 9.92 reward.\n",
      "Episode 1801: Agent gets 9.92 reward.\n",
      "Episode 1851: Agent gets 9.88 reward.\n",
      "Episode 1901: Agent gets 9.92 reward.\n",
      "Episode 1951: Agent gets 9.92 reward.\n",
      "Episode 2001: Agent gets 9.72 reward.\n",
      "Episode 2051: Agent gets 9.92 reward.\n",
      "Episode 2101: Agent gets 9.88 reward.\n",
      "Episode 2151: Agent gets 9.76 reward.\n",
      "Episode 2201: Agent gets 9.84 reward.\n",
      "Episode 2251: Agent gets 9.92 reward.\n",
      "Episode 2301: Agent gets 9.8 reward.\n",
      "Episode 2351: Agent gets 9.92 reward.\n",
      "Episode 2401: Agent gets 9.92 reward.\n",
      "Episode 2451: Agent gets 9.88 reward.\n",
      "Episode 2501: Agent gets 9.84 reward.\n",
      "Episode 2551: Agent gets 9.68 reward.\n",
      "Episode 2601: Agent gets 9.92 reward.\n",
      "Episode 2651: Agent gets 9.88 reward.\n",
      "Episode 2701: Agent gets 9.84 reward.\n",
      "Episode 2751: Agent gets 9.92 reward.\n",
      "Episode 2801: Agent gets 9.6 reward.\n",
      "Episode 2851: Agent gets 9.84 reward.\n",
      "Episode 2901: Agent gets 9.76 reward.\n",
      "Episode 2951: Agent gets 9.84 reward.\n",
      "Episode 3001: Agent gets 9.92 reward.\n",
      "Episode 3051: Agent gets 9.92 reward.\n",
      "Episode 3101: Agent gets 9.92 reward.\n",
      "Episode 3151: Agent gets 9.84 reward.\n",
      "Episode 3201: Agent gets 9.92 reward.\n",
      "Episode 3251: Agent gets 9.84 reward.\n",
      "Episode 3301: Agent gets 9.92 reward.\n",
      "Episode 3351: Agent gets 9.84 reward.\n",
      "Episode 3401: Agent gets 9.92 reward.\n",
      "Episode 3451: Agent gets 9.64 reward.\n",
      "Episode 3501: Agent gets -12.760000000000002 reward.\n",
      "Episode 3551: Agent gets 9.92 reward.\n",
      "Episode 3601: Agent gets 9.88 reward.\n",
      "Episode 3651: Agent gets 9.92 reward.\n",
      "Episode 3701: Agent gets 9.92 reward.\n",
      "Episode 3751: Agent gets 9.88 reward.\n",
      "Episode 3801: Agent gets 9.8 reward.\n",
      "Episode 3851: Agent gets 9.88 reward.\n",
      "Episode 3901: Agent gets 9.48 reward.\n",
      "Episode 3951: Agent gets 9.92 reward.\n",
      "Episode 4001: Agent gets 9.92 reward.\n",
      "Episode 4051: Agent gets 9.92 reward.\n",
      "Episode 4101: Agent gets 9.92 reward.\n",
      "Episode 4151: Agent gets 9.84 reward.\n",
      "Episode 4201: Agent gets 9.92 reward.\n",
      "Episode 4251: Agent gets 9.88 reward.\n",
      "Episode 4301: Agent gets 9.92 reward.\n",
      "Episode 4351: Agent gets 9.88 reward.\n",
      "Episode 4401: Agent gets 9.92 reward.\n",
      "Episode 4451: Agent gets 8.799999999999999 reward.\n",
      "Episode 4501: Agent gets 9.92 reward.\n",
      "Episode 4551: Agent gets 9.92 reward.\n",
      "Episode 4601: Agent gets 9.56 reward.\n",
      "Episode 4651: Agent gets 9.8 reward.\n",
      "Episode 4701: Agent gets 9.92 reward.\n",
      "Episode 4751: Agent gets 9.8 reward.\n",
      "Episode 4801: Agent gets 9.72 reward.\n",
      "Episode 4851: Agent gets 9.92 reward.\n",
      "Episode 4901: Agent gets 9.92 reward.\n",
      "Episode 4951: Agent gets 9.84 reward.\n",
      "Episode 5001: Agent gets 9.8 reward.\n",
      "Episode 5051: Agent gets 9.92 reward.\n",
      "Episode 5101: Agent gets 9.92 reward.\n",
      "Episode 5151: Agent gets 9.64 reward.\n",
      "Episode 5201: Agent gets 9.76 reward.\n",
      "Episode 5251: Agent gets 9.8 reward.\n",
      "Episode 5301: Agent gets 9.92 reward.\n",
      "Episode 5351: Agent gets 9.76 reward.\n",
      "Episode 5401: Agent gets 9.6 reward.\n",
      "Episode 5451: Agent gets 9.92 reward.\n",
      "Episode 5501: Agent gets 9.92 reward.\n",
      "Episode 5551: Agent gets 9.92 reward.\n",
      "Episode 5601: Agent gets 9.92 reward.\n",
      "Episode 5651: Agent gets 9.88 reward.\n",
      "Episode 5701: Agent gets 9.8 reward.\n",
      "Episode 5751: Agent gets 9.76 reward.\n",
      "Episode 5801: Agent gets 9.76 reward.\n",
      "Episode 5851: Agent gets 9.88 reward.\n",
      "Episode 5901: Agent gets 9.88 reward.\n",
      "Episode 5951: Agent gets 9.88 reward.\n",
      "Episode 6001: Agent gets 9.84 reward.\n",
      "Episode 6051: Agent gets 9.88 reward.\n",
      "Episode 6101: Agent gets 9.92 reward.\n",
      "Episode 6151: Agent gets 9.92 reward.\n",
      "Episode 6201: Agent gets 9.92 reward.\n",
      "Episode 6251: Agent gets 9.92 reward.\n",
      "Episode 6301: Agent gets 9.88 reward.\n",
      "Episode 6351: Agent gets -17.480000000000004 reward.\n",
      "Episode 6401: Agent gets 9.92 reward.\n",
      "Episode 6451: Agent gets 9.8 reward.\n",
      "Episode 6501: Agent gets 9.92 reward.\n",
      "Episode 6551: Agent gets 9.76 reward.\n",
      "Episode 6601: Agent gets 9.88 reward.\n",
      "Episode 6651: Agent gets 9.92 reward.\n",
      "Episode 6701: Agent gets 9.88 reward.\n",
      "Episode 6751: Agent gets 9.92 reward.\n",
      "Episode 6801: Agent gets 9.92 reward.\n",
      "Episode 6851: Agent gets -10.12 reward.\n",
      "Episode 6901: Agent gets 9.56 reward.\n",
      "Episode 6951: Agent gets 9.92 reward.\n",
      "Episode 7001: Agent gets 9.92 reward.\n",
      "Episode 7051: Agent gets 9.8 reward.\n",
      "Episode 7101: Agent gets 9.76 reward.\n",
      "Episode 7151: Agent gets 9.8 reward.\n",
      "Episode 7201: Agent gets 9.92 reward.\n",
      "Episode 7251: Agent gets 9.92 reward.\n",
      "Episode 7301: Agent gets 9.92 reward.\n",
      "Episode 7351: Agent gets 9.92 reward.\n",
      "Episode 7401: Agent gets 9.84 reward.\n",
      "Episode 7451: Agent gets 9.92 reward.\n",
      "Episode 7501: Agent gets 9.84 reward.\n",
      "Episode 7551: Agent gets 9.84 reward.\n",
      "Episode 7601: Agent gets 9.64 reward.\n",
      "Episode 7651: Agent gets 9.92 reward.\n",
      "Episode 7701: Agent gets 9.8 reward.\n",
      "Episode 7751: Agent gets 9.88 reward.\n",
      "Episode 7801: Agent gets 9.92 reward.\n",
      "Episode 7851: Agent gets 9.68 reward.\n",
      "Episode 7901: Agent gets 9.84 reward.\n",
      "Episode 7951: Agent gets 9.92 reward.\n",
      "Episode 8001: Agent gets 9.92 reward.\n",
      "Episode 8051: Agent gets 9.88 reward.\n",
      "Episode 8101: Agent gets 9.84 reward.\n",
      "Episode 8151: Agent gets 9.92 reward.\n",
      "Episode 8201: Agent gets 9.88 reward.\n",
      "Episode 8251: Agent gets 9.92 reward.\n",
      "Episode 8301: Agent gets 9.72 reward.\n",
      "Episode 8351: Agent gets 9.8 reward.\n",
      "Episode 8401: Agent gets 9.92 reward.\n",
      "Episode 8451: Agent gets 9.92 reward.\n",
      "Episode 8501: Agent gets 9.92 reward.\n",
      "Episode 8551: Agent gets 9.84 reward.\n",
      "Episode 8601: Agent gets 9.84 reward.\n",
      "Episode 8651: Agent gets 9.84 reward.\n",
      "Episode 8701: Agent gets 9.88 reward.\n",
      "Episode 8751: Agent gets 9.92 reward.\n",
      "Episode 8801: Agent gets 9.76 reward.\n",
      "Episode 8851: Agent gets 9.92 reward.\n",
      "Episode 8901: Agent gets 9.92 reward.\n",
      "Episode 8951: Agent gets 9.92 reward.\n",
      "Episode 9001: Agent gets 9.68 reward.\n",
      "Episode 9051: Agent gets 9.88 reward.\n",
      "Episode 9101: Agent gets 9.92 reward.\n",
      "Episode 9151: Agent gets 9.92 reward.\n",
      "Episode 9201: Agent gets 9.84 reward.\n",
      "Episode 9251: Agent gets 9.8 reward.\n",
      "Episode 9301: Agent gets 9.84 reward.\n",
      "Episode 9351: Agent gets 9.92 reward.\n",
      "Episode 9401: Agent gets 9.92 reward.\n",
      "Episode 9451: Agent gets 9.92 reward.\n",
      "Episode 9501: Agent gets 9.8 reward.\n",
      "Episode 9551: Agent gets 9.88 reward.\n",
      "Episode 9601: Agent gets 9.92 reward.\n",
      "Episode 9651: Agent gets 9.92 reward.\n",
      "Episode 9701: Agent gets 9.84 reward.\n",
      "Episode 9751: Agent gets 9.8 reward.\n",
      "Episode 9801: Agent gets 9.92 reward.\n",
      "Episode 9851: Agent gets 9.92 reward.\n",
      "Episode 9901: Agent gets 9.8 reward.\n",
      "Episode 9951: Agent gets 9.88 reward.\n"
     ]
    }
   ],
   "source": [
    "# Training全体を受け持つクラス\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, agent, env, episode=1, report_interval=50):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "\n",
    "        self.episode = episode\n",
    "        self.report_interval = report_interval\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.episode):\n",
    "            if i % self.report_interval == 0:\n",
    "                print(\"Episode {}: Agent gets {} reward.\".format(i+1, self.one_episode()))\n",
    "\n",
    "    def one_episode(self):\n",
    "        agent_state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = self.agent.act(agent_state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            agent_state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "class MonteCarloTrainer(Trainer):\n",
    "    def __init__(self, agent, env, episode=1, report_interval=50):\n",
    "        super().__init__(agent, env, episode)\n",
    "        self.report_interval = report_interval\n",
    "\n",
    "    def one_episode(self):\n",
    "        agent_state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.agent.init_log()\n",
    "            while not done:\n",
    "                a = self.agent.act(agent_state)\n",
    "                n_state, reward, done = self.env.step(a)\n",
    "                self.agent.experience_add( agent_state, a, reward)\n",
    "                total_reward += reward\n",
    "                agent_state = n_state\n",
    "            else:\n",
    "                self.agent.reward_add(reward)\n",
    "\n",
    "        self.agent.learn()\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 環境データ\n",
    "    grid = [\n",
    "        [0, 0, 0, 1],\n",
    "        [0, 0, 0, -1],\n",
    "        [9, 0, -1, -1],\n",
    "        [0, 0, 0, 0],\n",
    "    ]\n",
    "\n",
    "    env = Maze(grid)\n",
    "    agent = MonteCarloAgent(env, epsilon=0.3)\n",
    "\n",
    "    trainer = MonteCarloTrainer(agent, env, 10000)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
